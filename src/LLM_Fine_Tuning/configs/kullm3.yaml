bnb_config:
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: True
  load_in_4bit: True
dataset:
  path: /workspace/Legal_Specific_KoLLM/dataset/qa_train_data.json
lora_config:
  bias: none
  lora_alpha: 32
  lora_dropout: 0.05
  r: 4
  target_modules:
  - q_proj
  - up_proj
  - o_proj
  - k_proj
  - down_proj
  - gate_proj
  - v_proj
  task_type: CAUSAL_LM
model: nlpai-lab/KULLM3
training_args:
  evaluation_strategy: steps
  fp16: True
  gradient_accumulation_steps: 1
  learning_rate: 0.0001
  load_best_model_at_end: True
  logging_steps: 100
  num_train_epochs: 1
  per_device_eval_batch_size: 16
  per_device_train_batch_size: 16
  report_to: wandb
  save_strategy: steps
  save_total_limit: 5
  seed: 8421
  weight_decay: 0.01
